{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570f3dd8",
   "metadata": {},
   "source": [
    "# BBM409 : Introduction to Machine Learning Lab. Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbeb7948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama==0.4.4 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: numpy==1.21.2 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.21.2)\n",
      "Requirement already satisfied: pandas==1.3.3 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2021.3 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (2021.3)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.62.2 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.62.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b943e96",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185b7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2ed75",
   "metadata": {},
   "source": [
    "### Euclidean Distance Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c49bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(first_row, second_row):\n",
    "    if len(first_row) != len(second_row):\n",
    "        raise Exception(\"Two rows must have the same dimension!\")\n",
    "\n",
    "    total = 0\n",
    "    for index in range(len(first_row)):\n",
    "        diff_square = (first_row[index] - second_row[index]) ** 2\n",
    "        total += diff_square\n",
    "    return sqrt(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab56b2f",
   "metadata": {},
   "source": [
    "### Accuracy Calculator Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c55abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(predicted, labels):\n",
    "    predicted = np.array(predicted)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Comparison between predictions and the test labels.\n",
    "    accuracy = (predicted == labels).sum() / len(predicted)\n",
    "    # print(accuracy*100)\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882803aa",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Calculator Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3eef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(predicted, labels):\n",
    "    predicted = np.array(predicted)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    mae = np.absolute(labels - predicted).sum() / len(predicted)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56579048",
   "metadata": {},
   "source": [
    "### Min-Max Normalization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(data):\n",
    "    # max_bucket = [-1000000000 for i in range(len(data[0]))]\n",
    "\n",
    "    # working on temp data\n",
    "    temp_data = data.copy()\n",
    "\n",
    "    length = len(temp_data[0])\n",
    "\n",
    "    # scaling each attribute of the data\n",
    "    for i in range(length):\n",
    "        col_data = temp_data[:, i]\n",
    "        _max = np.max(col_data)\n",
    "        _min = np.min(col_data)\n",
    "\n",
    "        # applying min-max normalization\n",
    "        col_data = (col_data - _min) / (_max - _min)\n",
    "\n",
    "        # replace data\n",
    "        temp_data[:, i] = col_data\n",
    "\n",
    "    return temp_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ba269",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation Splitter Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d9a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_split(X, k=5):\n",
    "    # takes numpy array as an argument\n",
    "    # shuffle\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    splitted_data = []\n",
    "    data_partition_num = len(X) / k\n",
    "\n",
    "    for i in range(k):\n",
    "        start = int(i * data_partition_num)\n",
    "        end = int((i + 1) * data_partition_num)\n",
    "\n",
    "        test_set = X[start:end, :]\n",
    "        train_set = np.concatenate((X[0:start, :], X[end:, :]), axis=0)\n",
    "\n",
    "        splitted_data.append(np.array([train_set, test_set], dtype=object))\n",
    "\n",
    "    return np.array(splitted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6877ffb",
   "metadata": {},
   "source": [
    "### Method That Performs Cross Validation(runs all possible train set combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a469385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(splitted_data, knn, normalize, classification):\n",
    "    accuracies = []\n",
    "    mae_values = []\n",
    "\n",
    "    for data in splitted_data:\n",
    "        sample_train = data[0]\n",
    "        sample_test = data[1]\n",
    "\n",
    "        # print(sample_train.shape)\n",
    "        # print(sample_test.shape)\n",
    "\n",
    "        # train and test sets\n",
    "        X_train = sample_train[:, :-1]\n",
    "        y_train = sample_train[:, -1]\n",
    "\n",
    "        X_test = sample_test[:, :-1]\n",
    "        y_test = sample_test[:, -1]\n",
    "\n",
    "        if normalize:\n",
    "            \"\"\"\n",
    "                Applying min-max normalization to data\n",
    "\n",
    "                Applying separately to avoid data leakage\n",
    "            \"\"\"\n",
    "            X_train = min_max_normalization(X_train)\n",
    "            X_test = min_max_normalization(X_test)\n",
    "\n",
    "        # fitting data\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # prediction part\n",
    "        predictions = knn.predict(X_test)\n",
    "\n",
    "        # print(knn.y_train)\n",
    "        # print(predictions)\n",
    "        # print(y_test)\n",
    "\n",
    "        if classification:\n",
    "            # calculate accuracy\n",
    "            accuracy = accuracy_score(predictions, y_test)\n",
    "            accuracies.append(accuracy)\n",
    "        else:\n",
    "            mae = mean_absolute_error(predictions, y_test)\n",
    "            mae_values.append(mae)\n",
    "\n",
    "    if classification:\n",
    "        accuracies = np.array(accuracies)\n",
    "        return np.sort(accuracies), np.mean(accuracies)\n",
    "    else:\n",
    "        return np.sort(mae_values), np.mean(mae_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646bc5c",
   "metadata": {},
   "source": [
    "### KNN Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d1eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnBase:\n",
    "\n",
    "    # setting k value of KNN algorithm\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    # setting attribute and class data\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    # majority voting function\n",
    "    def vote(self, neighbours, frequence_array):\n",
    "        raise NotImplementedError(\"Each class must implement their own voting function !\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted_classes = [self.predict_helper(row) for row in X]\n",
    "        return predicted_classes\n",
    "\n",
    "    def predict_helper(self, x):\n",
    "        raise NotImplementedError(\"Each class must implement their own predict_helper function !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233f1fa",
   "metadata": {},
   "source": [
    "### KNN Classification Class Without Weight Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed756d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(KnnBase):\n",
    "\n",
    "    # setting k value of KNN algorithm\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__(k)\n",
    "\n",
    "    # majority voting function\n",
    "    def vote(self, neighbours, frequence_array=None):\n",
    "        distances = []\n",
    "        classes = []\n",
    "        indexes = []\n",
    "\n",
    "        for neighbour in neighbours:\n",
    "            index = neighbour[1]  # neighbour[1] : index of related data's row\n",
    "            # recall --> [dist, index_of_neighbour]\n",
    "\n",
    "            _class = self.y_train[index]  # _ class : corresponding data in the Class set\n",
    "            _distance = neighbour[0]  # neighbour[0] : distance of related data\n",
    "\n",
    "            # save all information in various lists\n",
    "            distances.append(_distance)\n",
    "            classes.append(_class)\n",
    "            indexes.append(index)\n",
    "\n",
    "        # print(\"-----voting part-----\")\n",
    "        counted = list(Counter(classes).items())  # numerical grouping by class of each data\n",
    "        # print(classes)\n",
    "        # print(counted)\n",
    "        # print(\"distances: \", distances)\n",
    "\n",
    "        most_common = counted[0]  # find the most repetitive class\n",
    "        class_numbers = [i[1] for i in counted]  # store the number of repetitions of classes\n",
    "        # print(class_numbers)\n",
    "\n",
    "        if class_numbers.count(most_common[0]) > 1:  # if the most common class has more than one sample\n",
    "\n",
    "            # x = input(\"tie found, press any key to continue\")\n",
    "\n",
    "            # that means we have a tie situation..\n",
    "            classes_of_duplicate_occurrences = []\n",
    "            for cnt in counted:\n",
    "                if cnt[1] == most_common[1]:\n",
    "                    # the one that we are looking for\n",
    "                    classes_of_duplicate_occurrences.append(\n",
    "                        cnt[0])  # find the classes of the data that provide the tie situation\n",
    "\n",
    "            # print(classes_of_duplicate_occurrences)\n",
    "\n",
    "            # breaking tie part\n",
    "            new_distances = [[] for i in range(int(max(classes_of_duplicate_occurrences)) + 1)]\n",
    "\n",
    "            for _class in classes_of_duplicate_occurrences:\n",
    "                for __index, __class in enumerate(classes):\n",
    "                    if _class == __class:\n",
    "                        # access distance of neighbour from the array which is created above\n",
    "                        indiv_distance = distances[__index]\n",
    "\n",
    "                        # store distances to calculate sums later and decide nearest neighbour\n",
    "                        new_distances[int(_class)].append(indiv_distance)\n",
    "\n",
    "            # print(new_distances)\n",
    "\n",
    "            empty_list_indices = [i for i in range(len(new_distances)) if len(new_distances[i]) == 0]\n",
    "\n",
    "            for index, new_distance in enumerate(new_distances):\n",
    "                new_distances[index] = sum(new_distance)\n",
    "\n",
    "            new_distances = np.array(new_distances)\n",
    "            # print(new_distances)\n",
    "\n",
    "            sorted_indices = np.argsort(new_distances)\n",
    "            # print(sorted_indices)\n",
    "\n",
    "            # print(\"empty part\")\n",
    "            # print(empty_list_indices)\n",
    "\n",
    "            will_be_deleted_indices = []\n",
    "            for index in empty_list_indices:\n",
    "                for i, _index in enumerate(sorted_indices):\n",
    "                    if index == _index:\n",
    "                        will_be_deleted_indices.append(i)\n",
    "\n",
    "            sorted_indices = np.delete(sorted_indices, will_be_deleted_indices)\n",
    "            # print(sorted_indices)\n",
    "\n",
    "            predicted_class = int(sorted_indices[0])\n",
    "            # print(predicted_class)\n",
    "        else:\n",
    "            # no tie select most common as nearest neighbour\n",
    "            predicted_class = int(most_common[0])\n",
    "\n",
    "        return predicted_class\n",
    "\n",
    "    def predict_helper(self, x):\n",
    "        \"\"\"\n",
    "        [\n",
    "            [dist, index_of_neighbour],\n",
    "            [another_dist, index_of_another_neighbour],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        # distance calculation\n",
    "        distances = [[euclidean_distance(x, self.X_train[i]), i] for i in range(len(self.X_train))]\n",
    "\n",
    "        # sorting to find nearest neighbour\n",
    "        # print(distances)\n",
    "        sorted_array = sorted(distances, key=lambda x: x[0])\n",
    "        # print(sorted_array)\n",
    "\n",
    "        # print(\"Asked data: \", x.astype(int))\n",
    "        # print(\"Nearest Neighbours:\")\n",
    "        for m in range(self.k):\n",
    "            index = sorted_array[m][1]\n",
    "            # print(self.X_train[index], \", class: \" ,self.y_train[index])\n",
    "\n",
    "        predicted = self.vote(sorted_array[:self.k])\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b8d5e",
   "metadata": {},
   "source": [
    "### KNN Classification Class With Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f33f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKNN(KnnBase):\n",
    "\n",
    "    # setting k value of KNN algorithm\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__(k)\n",
    "\n",
    "    # majority voting function\n",
    "    def vote(self, neighbours, frequence_array):\n",
    "        distances = []\n",
    "        classes = []\n",
    "        indexes = []\n",
    "        weights = []\n",
    "\n",
    "        for neighbour in neighbours:\n",
    "            _index = neighbour[1]  # neighbour[1] : index of related data's row\n",
    "            # recall --> [dist, index_of_neighbour]\n",
    "\n",
    "            _class = self.y_train[_index]  # _ class : corresponding data in the Class set\n",
    "            _distance = neighbour[0]  # neighbour[0] : distance of related data\n",
    "            _weight = 1 if _distance == 0 else (1 / _distance)\n",
    "\n",
    "            # save all information in various lists\n",
    "            distances.append(_distance)\n",
    "            classes.append(_class)\n",
    "            indexes.append(_index)\n",
    "            weights.append(_weight)\n",
    "\n",
    "        # just for convention\n",
    "        counted = frequence_array\n",
    "\n",
    "        # print(\"-----voting part-----\")\n",
    "\n",
    "        # old code\n",
    "        # counted = list(Counter(classes).items())\n",
    "\n",
    "        # print(classes)\n",
    "        # print(counted)\n",
    "        # print(\"distances: \", distances)\n",
    "\n",
    "        most_common = counted[0]  # find the most repetitive class\n",
    "        class_numbers = [i[1] for i in counted]  # store the number of repetitions of classes\n",
    "        # print(class_numbers)\n",
    "\n",
    "        if class_numbers.count(most_common[0]) > 1:  # if the most common class has more than one sample\n",
    "\n",
    "            # x = input(\"tie found, press any key to continue\")\n",
    "\n",
    "            # that means we have a tie situation\n",
    "            classes_of_duplicate_occurrences = []\n",
    "            for cnt in counted:\n",
    "                if cnt[1] == most_common[1]:\n",
    "                    # the one that we are looking for\n",
    "                    classes_of_duplicate_occurrences.append(cnt[0])\n",
    "\n",
    "            # print(classes_of_duplicate_occurrences)\n",
    "\n",
    "            # breaking tie part\n",
    "            new_distances = [[] for i in range(int(max(classes_of_duplicate_occurrences)) + 1)]\n",
    "\n",
    "            for _class in classes_of_duplicate_occurrences:\n",
    "                for __index, __class in enumerate(classes):\n",
    "                    if _class == __class:\n",
    "                        # access distance of neighbour from the array which is created above\n",
    "                        indiv_distance = distances[__index]\n",
    "\n",
    "                        # store distances to calculate sums later and decide nearest neighbour\n",
    "                        new_distances[int(_class)].append(indiv_distance)\n",
    "\n",
    "            # print(new_distances)\n",
    "\n",
    "            empty_list_indices = [i for i in range(len(new_distances)) if len(new_distances[i]) == 0]\n",
    "\n",
    "            for index, new_distance in enumerate(new_distances):\n",
    "                new_distances[index] = sum(new_distance)\n",
    "\n",
    "            new_distances = np.array(new_distances)\n",
    "            # print(new_distances)\n",
    "\n",
    "            sorted_indices = np.argsort(new_distances)\n",
    "            # print(sorted_indices)\n",
    "\n",
    "            # print(\"empty part\")\n",
    "            # print(empty_list_indices)\n",
    "\n",
    "            will_be_deleted_indices = []\n",
    "            for index in empty_list_indices:\n",
    "                for i, _index in enumerate(sorted_indices):\n",
    "                    if index == _index:\n",
    "                        will_be_deleted_indices.append(i)\n",
    "\n",
    "            sorted_indices = np.delete(sorted_indices, will_be_deleted_indices)\n",
    "            # print(sorted_indices)\n",
    "\n",
    "            predicted_class = int(sorted_indices[0])\n",
    "            # print(predicted_class)\n",
    "        else:\n",
    "            # no tie select most common as nearest neighbour\n",
    "            predicted_class = int(most_common[0])\n",
    "\n",
    "        return predicted_class\n",
    "\n",
    "    def predict_helper(self, x):\n",
    "        \"\"\"\n",
    "        [\n",
    "            [dist, index_of_neighbour],\n",
    "            [another_dist, index_of_another_neighbour],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        # distance calculation\n",
    "        distances = [[euclidean_distance(x, self.X_train[i]), i] for i in range(len(self.X_train))]\n",
    "\n",
    "        # sorting to find nearest neighbour\n",
    "        # print(distances)\n",
    "        sorted_array = sorted(distances, key=lambda x: x[0])\n",
    "        # print(sorted_array)\n",
    "\n",
    "        # print(\"Asked data: \", x.astype(int))\n",
    "        # print(\"Nearest Neighbours:\")\n",
    "        for m in range(self.k):\n",
    "            index = sorted_array[m][1]\n",
    "            # print(self.X_train[index], \", class: \" ,self.y_train[index])\n",
    "\n",
    "        # finding maximum class number to decide weight_dict size\n",
    "        maximum_class = int(np.max([self.y_train[i[1]] for i in sorted_array[:self.k]]))\n",
    "\n",
    "        weight_dict = {(i + 1): 0 for i in range(maximum_class + 1)}\n",
    "\n",
    "        for _tuple in range(self.k):\n",
    "            neighb_val = self.y_train[sorted_array[_tuple][1]]\n",
    "            # print(sorted_array[_tuple][0])\n",
    "            if not sorted_array[_tuple][0] == 0:\n",
    "                weight_dict[neighb_val] += (1 / sorted_array[_tuple][0])\n",
    "            else:\n",
    "                # if distance value equals to 0, weight value becomes 1 to avoid zero division error\n",
    "                weight_dict[neighb_val] += 1\n",
    "\n",
    "        # equivalent of Counter() method output from collections which has used on basic KNN\n",
    "        \"\"\"\n",
    "        frequence_array = [\n",
    "            (class1, weight1), \n",
    "            (class2, weight2),\n",
    "            (..., ...),\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        frequence_array = [(_class, weight_dict[_class]) for _class in weight_dict.keys()]\n",
    "        frequence_array = sorted(frequence_array, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print(frequence_array)\n",
    "\n",
    "        # print(sorted_array[:self.k])\n",
    "        predicted = self.vote(sorted_array[:self.k], frequence_array)\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccd497",
   "metadata": {},
   "source": [
    "### KNN Regression Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14329885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressionBase:\n",
    "\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def regression(self, neighbours):\n",
    "        raise NotImplementedError(\"Each class must implement their regression function !\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted_classes = [self.predict_helper(row) for row in X]\n",
    "        return predicted_classes\n",
    "\n",
    "    def predict_helper(self, x):\n",
    "        \"\"\"\n",
    "        [\n",
    "            [dist, index_of_neighbour],\n",
    "            [another_dist, index_of_another_neighbour],\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        # distance calculation\n",
    "        distances = [[euclidean_distance(x, self.X_train[i]), i] for i in range(len(self.X_train))]\n",
    "\n",
    "        # sorting to find nearest neighbour\n",
    "        # print(distances)\n",
    "        sorted_array = sorted(distances, key=lambda x: x[0])\n",
    "        # print(sorted_array)\n",
    "\n",
    "        # print(\"Asked data: \", x.astype(int))\n",
    "        # print(\"Nearest Neighbours:\")\n",
    "        for m in range(self.k):\n",
    "            index = sorted_array[m][1]\n",
    "            # print(self.X_train[index], \", class: \" ,self.y_train[index])\n",
    "\n",
    "        predicted = self.regression(sorted_array[:self.k])\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c20947",
   "metadata": {},
   "source": [
    "### KNN Regression Class Without Weight Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4e9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegression(KNNRegressionBase):\n",
    "\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__(k)\n",
    "\n",
    "    def regression(self, neighbours):\n",
    "        values = [self.y_train[neighbour[1]] for neighbour in neighbours]\n",
    "        return np.mean(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b7a0a",
   "metadata": {},
   "source": [
    "### KNN Regression Class With Weight Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54797e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKNNRegression(KNNRegressionBase):\n",
    "\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__(k)\n",
    "\n",
    "    def regression(self, neighbours):\n",
    "        values = np.array([self.y_train[neighbour[1]] for neighbour in neighbours])\n",
    "        weights = np.array([(1 / neighbour[0]) if neighbour[0] != 0 else 1 for neighbour in neighbours])\n",
    "\n",
    "        return ((values * weights).sum()) / weights.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e27e3c",
   "metadata": {},
   "source": [
    "### Test Method That Performs Several Tests Which Are All Possible Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8242515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalTest():\n",
    "    \n",
    "    # classification part\n",
    "    classification_data = pd.read_csv(\"glass.csv\")\n",
    "    classification_data = np.array(classification_data)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    splitted = k_fold_cross_validation_split(classification_data, 5)\n",
    "\n",
    "    # regression part\n",
    "    regression_data = pd.read_csv(\"Concrete_Data_Yeh.csv\")\n",
    "    regression_data = np.array(regression_data)\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    regression_splitted = k_fold_cross_validation_split(regression_data, 5)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1,6):\n",
    "        k_val = (2 * i) - 1\n",
    "\n",
    "        for bool in [True, False]:\n",
    "            text = \"With\" if bool else \"Without\"\n",
    "            print(\"*********************************\")\n",
    "            print(f\"{text} Normalization...\")\n",
    "            print(f\"With k={k_val}...\")\n",
    "\n",
    "            print(\"Classification Part:\")\n",
    "            knn = KNN(k=k_val)\n",
    "            knn1 = WeightedKNN(k=k_val)\n",
    "\n",
    "            accuracies = cross_validation(splitted, knn, normalize=bool, classification=True)\n",
    "            accuracies1 = cross_validation(splitted, knn1, normalize=bool, classification=True)\n",
    "\n",
    "\n",
    "            print(\"Distance based accuracies: \", accuracies)\n",
    "            print(\"Weighted accuracies: \", accuracies1)\n",
    "\n",
    "            print(\"Regression Part:\")\n",
    "            knn2 = KNNRegression(k=k_val)\n",
    "            knn3 = WeightedKNNRegression(k=k_val)\n",
    "\n",
    "            mae_values = cross_validation(regression_splitted, knn2, normalize=bool, classification=False)\n",
    "            mae_values1 = cross_validation(regression_splitted, knn3, normalize=bool, classification=False)\n",
    "\n",
    "            print(\"Mae values: \", mae_values)\n",
    "            print(\"Weighted Mae values: \", mae_values1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c0f2d",
   "metadata": {},
   "source": [
    "#### Running General Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e247347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************\n",
      "With Normalization...\n",
      "With k=1...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Weighted accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.14679612, 6.51538835, 6.67815534, 6.96286408, 7.44087379]), 6.748815533980581)\n",
      "Weighted Mae values:  (array([6.14679612, 6.51538835, 6.67815534, 6.96286408, 7.44087379]), 6.748815533980583)\n",
      "*********************************\n",
      "Without Normalization...\n",
      "With k=1...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([65.11627907, 67.44186047, 71.42857143, 72.09302326, 72.09302326]), 69.63455149501661)\n",
      "Weighted accuracies:  (array([65.11627907, 67.44186047, 71.42857143, 72.09302326, 72.09302326]), 69.63455149501661)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.0131068 , 6.21796117, 6.4781068 , 6.77961165, 7.06597087]), 6.51095145631068)\n",
      "Weighted Mae values:  (array([6.0131068 , 6.21796117, 6.4781068 , 6.77961165, 7.06597087]), 6.51095145631068)\n",
      "*********************************\n",
      "With Normalization...\n",
      "With k=3...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Weighted accuracies:  (array([53.48837209, 58.13953488, 64.28571429, 65.11627907, 67.44186047]), 61.69435215946844)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.75456311, 6.90665049, 6.96877023, 6.97360841, 7.07770227]), 6.936258899676375)\n",
      "Weighted Mae values:  (array([6.32868668, 6.54509387, 6.62954947, 6.64456586, 6.73660016]), 6.57689920857883)\n",
      "*********************************\n",
      "Without Normalization...\n",
      "With k=3...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([65.11627907, 67.44186047, 71.42857143, 72.09302326, 72.09302326]), 69.63455149501661)\n",
      "Weighted accuracies:  (array([53.48837209, 67.44186047, 69.76744186, 71.42857143, 72.09302326]), 66.84385382059801)\n",
      "Regression Part:\n",
      "Mae values:  (array([5.93805825, 6.64794498, 6.72690939, 6.82771845, 7.01148867]), 6.630423948220065)\n",
      "Weighted Mae values:  (array([5.07175691, 5.42786986, 5.53485039, 5.87055093, 6.32444581]), 5.645894780868307)\n",
      "*********************************\n",
      "With Normalization...\n",
      "With k=5...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Weighted accuracies:  (array([48.8372093 , 61.9047619 , 62.79069767, 62.79069767, 62.79069767]), 59.82281284606866)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.80321359, 6.91772816, 7.02267961, 7.35825243, 7.79660194]), 7.179695145631068)\n",
      "Weighted Mae values:  (array([6.29356496, 6.64714256, 6.89167939, 7.11952225, 7.3574277 ]), 6.86186737096933)\n",
      "*********************************\n",
      "Without Normalization...\n",
      "With k=5...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([65.11627907, 67.44186047, 71.42857143, 72.09302326, 72.09302326]), 69.63455149501661)\n",
      "Weighted accuracies:  (array([60.46511628, 65.11627907, 66.66666667, 69.76744186, 74.41860465]), 67.28682170542636)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.49723301, 6.71380583, 6.8146699 , 6.8367767 , 7.38074757]), 6.848646601941748)\n",
      "Weighted Mae values:  (array([5.19083838, 5.32426485, 5.55544637, 5.8616284 , 6.01394501]), 5.589224598953561)\n",
      "*********************************\n",
      "With Normalization...\n",
      "With k=7...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Weighted accuracies:  (array([44.18604651, 58.13953488, 59.52380952, 62.79069767, 65.11627907]), 57.95127353266888)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.77339112, 6.99866158, 7.29515257, 7.46134535, 8.0187656 ]), 7.309463245492371)\n",
      "Weighted Mae values:  (array([6.26480189, 6.85340739, 6.91363111, 7.06704822, 7.5623521 ]), 6.932248140389952)\n",
      "*********************************\n",
      "Without Normalization...\n",
      "With k=7...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([65.11627907, 65.11627907, 71.42857143, 72.09302326, 72.09302326]), 69.16943521594683)\n",
      "Weighted accuracies:  (array([58.13953488, 61.9047619 , 65.11627907, 69.76744186, 76.74418605]), 66.3344407530454)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.70635922, 7.05432039, 7.08094313, 7.21988904, 7.70522885]), 7.153348127600554)\n",
      "Weighted Mae values:  (array([5.20479143, 5.42221135, 5.57332863, 5.96731354, 6.06045032]), 5.645619053580825)\n",
      "*********************************\n",
      "With Normalization...\n",
      "With k=9...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([53.48837209, 58.13953488, 59.52380952, 60.46511628, 62.79069767]), 58.88150609080841)\n",
      "Weighted accuracies:  (array([44.18604651, 50.        , 51.1627907 , 58.13953488, 58.13953488]), 52.32558139534884)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.86331715, 7.26026429, 7.63624056, 7.72251348, 8.22220065]), 7.540907227615965)\n",
      "Weighted Mae values:  (array([6.23536849, 6.98135881, 7.02321548, 7.42774505, 7.74535382]), 7.082608330018391)\n",
      "*********************************\n",
      "Without Normalization...\n",
      "With k=9...\n",
      "Classification Part:\n",
      "Distance based accuracies:  (array([65.11627907, 67.44186047, 71.42857143, 72.09302326, 72.09302326]), 69.63455149501661)\n",
      "Weighted accuracies:  (array([53.48837209, 57.14285714, 67.44186047, 69.76744186, 72.09302326]), 63.98671096345514)\n",
      "Regression Part:\n",
      "Mae values:  (array([6.80943366, 7.1069849 , 7.60710895, 7.61019417, 7.89848975]), 7.406442286947142)\n",
      "Weighted Mae values:  (array([5.07920401, 5.54581047, 5.64413928, 6.01362314, 6.2734762 ]), 5.711250620955375)\n"
     ]
    }
   ],
   "source": [
    "generalTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
